{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12674718,"sourceType":"datasetVersion","datasetId":8009813}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:47:01.904577Z","iopub.execute_input":"2025-11-10T11:47:01.905253Z","iopub.status.idle":"2025-11-10T11:47:03.496794Z","shell.execute_reply.started":"2025-11-10T11:47:01.905230Z","shell.execute_reply":"2025-11-10T11:47:03.495913Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_216.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_98.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_137.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_86_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_19_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_202.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_159.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_217.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_185_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_40_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_107.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_37.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_218_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_140_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_32_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_190.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_39.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_212.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_173.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_70_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_77.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_44.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_0.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_154.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_98_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_34.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videossssstttsss_48.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_130.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_96_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_123.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_93.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_16_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_47.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_24_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_213_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_214_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_36.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_170.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_35.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_7_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_49.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_169.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_80_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_46.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_56.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_123_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_80.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_3.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_120.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_172.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_101.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_11.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_121_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_67.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_31_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_211.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_27.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_151_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_117_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_72.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_113_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_29.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_12.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_130_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_199_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_10.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_61_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_124.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_40.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_168_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_42.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_14.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_122.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_201.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_65.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_52_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_13.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_4.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_179_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_162_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_78.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_191_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_178.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_75_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_194.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_195.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_63_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_54_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_202_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_63.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_53_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_165_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_106.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_36.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_118.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_79_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_158_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_27_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_39.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_194_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_21.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_29.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_76.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_107_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_59.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_134.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_82_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_177.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_119.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_154_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_22.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_100.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_25_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_115_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_92.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_211_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_156.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_213.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_167_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_181.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_12.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_63.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_76.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_197.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_75.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_163.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_156_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_26.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_49_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_207.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_64.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_136_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videossssstttsss_53.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_2.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_104_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_47.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_2.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_104.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_29_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_110.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_149_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_218.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_56_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_0.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_180_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_24.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_16.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_166.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_180.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_196.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_96.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_32.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_26.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_53.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_18_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_82.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_2_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_146_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_46_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_66.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_45.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_155.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_116_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_4_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_139_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_184_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_56.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_209.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_152.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_8.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_109.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_198.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_59.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_214.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_94_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_113.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_64_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_83_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_70.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_131.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_37.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_69_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_57.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_197_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_50.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_110_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_68_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_62.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_129.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_6.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_157_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_3.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_10_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_147_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_78.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_45.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_73.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_25.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_164_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_111.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_192.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_132_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_68.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_94.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_198_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_31.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_54.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_51_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_205_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_210_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_151.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_86.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_217_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_124_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_90.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_187.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_60.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videossssstttsss_51.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_39_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_11_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_160_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_176.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_23.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_20_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_42_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_84.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_200_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_89.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_22_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_167.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_195_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_8_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_8.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_186_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_142_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_108.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_79.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videossssstttsss_54.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_91_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_18.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_11.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_133.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_138.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_97.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_84.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_103.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_19.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_179.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_131_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_62_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_207_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_10.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_125_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_85.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_150_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_190_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_115.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_141_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_153.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_140.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_148.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_73_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_209_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_65.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_67.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_172_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_95.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_67_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_74.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_21_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_70.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_13_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_17_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_57.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_59_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_128_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_5.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_128.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_100_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_181_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_203_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_118_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_88.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_54.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_127_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_106_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_42.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videossssstttsss_55.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_120_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_174.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_146.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_55.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_30.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_157.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_43.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_55.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_33_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_185.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_145_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_116.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_216_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_206.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_81.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_99.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_114.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_57_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_134_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_117.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_112.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_68.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_62.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_46.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_165.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videossssstttsss_49.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_7.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_20.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_17.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_189.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_20.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_127.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_164.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_132.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_129_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_43_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_71.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_1_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_69.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_161.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_149.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_169_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_163_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_49.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_126.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_81_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_182.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_38.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_88_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_52.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_12_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_22.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_36_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_136.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_199.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_83.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_40.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_58_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_15.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_186.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_95_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_44.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_41.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_193_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_174_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_74_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_121.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_30.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_53.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_84_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_64.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_135.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_82.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_27.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_133_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_65_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_35.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_109_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_9.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_80.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_34.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_112_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_73.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_114_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_97_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_138_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_150.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_175.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_15_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_33.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_38.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_66.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_208.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_141.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_187_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_47_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_26_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_122_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_161_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_25.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_152_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_30_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_182_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_215_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_9_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_144.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_93_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_188_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_77_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_48.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_177_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_102_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videossssstttsss_52.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_43.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_160.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_191.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_75.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_87.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_183.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videossssstttsss_56.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_176_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_58.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_7.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_76_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_0_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_103_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_44_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_21.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_45_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_74.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_19.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_204.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_35_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_58.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_148_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_142.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_173_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_79.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_9.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_166_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_24.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_105.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_50.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_34_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_178_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_210.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_87_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_14.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_144_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_158.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_105_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_155_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_162.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_18.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_188.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_51.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_206_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_145.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_48_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_89_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_51.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_205.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_37_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_28.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videossssstttsss_50.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_6.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_28_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_61.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_119_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_91.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_33.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_72.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_108_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_13.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_60_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_208_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_77.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_90_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_48.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_4.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_175_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_201_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_6_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_72_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_31.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_17.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_66_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_168.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_41_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_183_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_189_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_55_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_200.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_15.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_50_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_5.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_38_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_111_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_69.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_192_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_92_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_41.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_71.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_126_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_5_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_60.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_203.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_101_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_14_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_102.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_16.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_193.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_85.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_143_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_125.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_52.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_159_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_135_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_153_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_28.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_212_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_147.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_23_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_83.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_71_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_3_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_215.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_81.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_32.mp4\n/kaggle/input/Shop DataSet/non shop lifters/videppppsss_23.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_170_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_99_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_85_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_78_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_204_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_184.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_139.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_137_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_143.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_196_1.mp4\n/kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_61.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_75.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_0.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_83.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_23.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_37.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_50.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_5.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_8.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_18.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_39.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_56.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_35.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_12.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_0.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_31.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_48.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_34.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_39.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_39.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_47.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_18.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_51.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_53.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_8.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_99.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_126.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_46.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_1.mp4\n/kaggle/input/Shop DataSet/shop lifters/videmmmmmmsss_91.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_35.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_59.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_46.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_34.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_3.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_5.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_4.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_47.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_3.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_87.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_13.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_23.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_8.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_12.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_44.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_10.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_40.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_1.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_29.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_6.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_62.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_20.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_42.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_90.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_12.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_95.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_58.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_22.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_36.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_77.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_21.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_29.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_51.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_25.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_94.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_3.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_45.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_11.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_17.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_42.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_89.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_26.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_67.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_125.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_32.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_2.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_38.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_64.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_78.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_16.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_5.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_53.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_26.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_53.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_81.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_54.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_109.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_17.mp4\n/kaggle/input/Shop DataSet/shop lifters/videmmmmmmsss_89.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_13.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_62.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_40.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_57.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_88.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_36.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_16.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_50.mp4\n/kaggle/input/Shop DataSet/shop lifters/videmmmmmmsss_88.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_14.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_25.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_6.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_60.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_2.mp4\n/kaggle/input/Shop DataSet/shop lifters/videmmmmmmsss_99.mp4\n/kaggle/input/Shop DataSet/shop lifters/videmmmmmmsss_85.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_45.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_45.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_101.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_19.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_4.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_25.mp4\n/kaggle/input/Shop DataSet/shop lifters/videmmmmmmsss_87.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_28.mp4\n/kaggle/input/Shop DataSet/shop lifters/videmmmmmmsss_95.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_36.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_52.mp4\n/kaggle/input/Shop DataSet/shop lifters/videmmmmmmsss_101.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_54.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_93.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_16.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_16.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_6.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_27.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_52.mp4\n/kaggle/input/Shop DataSet/shop lifters/videmmmmmmsss_92.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_7.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_59.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_33.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_31.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_119.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_20.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_6.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_120.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_20.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_26.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_35.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_8.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_27.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_18.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_11.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_114.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_25.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_24.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_117.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_86.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_19.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_32.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_49.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_70.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_107.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_9.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_13.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_18.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_127.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_48.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_123.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_61.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_50.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_5.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_10.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_92.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_49.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_30.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_112.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_61.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_42.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_21.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_47.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_19.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_34.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_16.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_55.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_12.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_115.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_15.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_29.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_41.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_43.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_116.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_7.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_41.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_124.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_105.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_5.mp4\n/kaggle/input/Shop DataSet/shop lifters/videmmmmmmsss_93.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_3.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_9.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_11.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_15.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_38.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_60.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_66.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_20.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_17.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_103.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_104.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_29.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_49.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_21.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_17.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_10.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_43.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_7.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_38.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_14.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_85.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_22.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_17.mp4\n/kaggle/input/Shop DataSet/shop lifters/videmmmmmmsss_97.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_0.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_102.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_15.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_21.mp4\n/kaggle/input/Shop DataSet/shop lifters/videmmmmmmsss_96.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_44.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_46.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_69.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_18.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_30.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_21.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_54.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_27.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_110.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_72.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_10.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_37.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_9.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_12.mp4\n/kaggle/input/Shop DataSet/shop lifters/videmmmmmmsss_100.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_108.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_10.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_9.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_37.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_68.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_22.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_0.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_8.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_23.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_30.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_15.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_1.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_76.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_24.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_113.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_84.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_9.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_26.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_4.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_111.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_43.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_28.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_80.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_7.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_98.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_19.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_1.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_57.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_97.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_19.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_55.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_63.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_100.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_58.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_24.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_24.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_14.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_2.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_14.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_51.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_64.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_23.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_13.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_20.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_55.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_96.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_28.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_71.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_33.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_122.mp4\n/kaggle/input/Shop DataSet/shop lifters/videmmmmmmsss_102.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_65.mp4\n/kaggle/input/Shop DataSet/shop lifters/videmmmmmmsss_94.mp4\n/kaggle/input/Shop DataSet/shop lifters/videmmmmmmsss_86.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_33.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_3.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_13.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_1.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_48.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_4.mp4\n/kaggle/input/Shop DataSet/shop lifters/videmmmmmmsss_98.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_11.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_7.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_31.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_22.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_79.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_121.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_40.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_14.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_27.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_4.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_6.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_41.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_24.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_106.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_74.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_22.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_28.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_128.mp4\n/kaggle/input/Shop DataSet/shop lifters/videmmmmmmsss_90.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_2.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyynnnnnnzzzzzyyyss_0.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_82.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_118.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_52.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_2.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_73.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyzzzzzyyyss_15.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_44.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_63.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_56.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_91.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_32.mp4\n/kaggle/input/Shop DataSet/shop lifters/videppppsss_23.mp4\n/kaggle/input/Shop DataSet/shop lifters/shop_lifter_65.mp4\n/kaggle/input/Shop DataSet/shop lifters/videyyyyyyyyyss_11.mp4\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!nvidia-smi\n!python -c \"import torch; print(torch.version.cuda); print(torch.cuda.get_device_name(0)); print(torch.cuda.get_device_capability(0))\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:47:03.498101Z","iopub.execute_input":"2025-11-10T11:47:03.498385Z","iopub.status.idle":"2025-11-10T11:47:06.034669Z","shell.execute_reply.started":"2025-11-10T11:47:03.498363Z","shell.execute_reply":"2025-11-10T11:47:06.033874Z"}},"outputs":[{"name":"stdout","text":"Mon Nov 10 11:47:03 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   38C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   38C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n12.8\nTesla T4\n(7, 5)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!pip install protobuf==3.20.3 --force-reinstall --no-cache-dir\n!pip install transformers==4.44.2 accelerate==0.33.0 decord==0.6.0 torch torchvision --upgrade --no-cache-dir\n!pip uninstall tensorflow -y\n!pip install tensorflow-cpu==2.12.0 --no-cache-dir  # optional: avoids conflicts with HF or protobuf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:47:06.035748Z","iopub.execute_input":"2025-11-10T11:47:06.036028Z","iopub.status.idle":"2025-11-10T11:47:19.804688Z","shell.execute_reply.started":"2025-11-10T11:47:06.036004Z","shell.execute_reply":"2025-11-10T11:47:19.803511Z"}},"outputs":[{"name":"stdout","text":"Collecting protobuf==3.20.3\n  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\nDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ntensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\norbax-checkpoint 0.11.19 requires jax>=0.5.0, but you have jax 0.4.30 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.20.3\nRequirement already satisfied: transformers==4.44.2 in /usr/local/lib/python3.11/dist-packages (4.44.2)\nRequirement already satisfied: accelerate==0.33.0 in /usr/local/lib/python3.11/dist-packages (0.33.0)\nRequirement already satisfied: decord==0.6.0 in /usr/local/lib/python3.11/dist-packages (0.6.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.9.0)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.24.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (2.32.5)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (0.5.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.2) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.33.0) (7.1.3)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\nRequirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.93)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.90)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.90)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.3.83)\nRequirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.9.90)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.3.90)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.8.93)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.27.5)\nRequirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.20)\nRequirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.90)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.93)\nRequirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1.3)\nRequirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.5.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (1.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.2) (2025.10.5)\n\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: tensorflow-cpu==2.12.0 in /usr/local/lib/python3.11/dist-packages (2.12.0)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu==2.12.0) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu==2.12.0) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu==2.12.0) (25.2.10)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu==2.12.0) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu==2.12.0) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu==2.12.0) (1.74.0)\nRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu==2.12.0) (3.14.0)\nRequirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu==2.12.0) (0.4.30)\nRequirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu==2.12.0) (2.12.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu==2.12.0) (18.1.1)\nRequirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu==2.12.0) (1.23.5)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu==2.12.0) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu==2.12.0) (25.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu==2.12.0) (3.20.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu==2.12.0) (75.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu==2.12.0) (1.17.0)\nRequirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu==2.12.0) (2.12.3)\nRequirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu==2.12.0) (2.12.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu==2.12.0) (3.1.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu==2.12.0) (4.15.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu==2.12.0) (1.14.2)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-cpu==2.12.0) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow-cpu==2.12.0) (0.45.1)\nRequirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow-cpu==2.12.0) (0.4.30)\nRequirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow-cpu==2.12.0) (0.4.1)\nRequirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow-cpu==2.12.0) (1.15.3)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (2.38.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (3.8.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (2.32.5)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (3.1.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (4.9.1)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (2.0.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (2025.10.5)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (3.0.3)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (0.6.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-cpu==2.12.0) (3.3.1)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, f1_score, accuracy_score, roc_curve, roc_auc_score, precision_recall_curve, auc\nimport matplotlib.pyplot as plt\nfrom decord import VideoReader, cpu\nfrom transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:51:56.246688Z","iopub.execute_input":"2025-11-10T11:51:56.247237Z","iopub.status.idle":"2025-11-10T11:51:56.251810Z","shell.execute_reply.started":"2025-11-10T11:51:56.247210Z","shell.execute_reply":"2025-11-10T11:51:56.250968Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:52:01.064662Z","iopub.execute_input":"2025-11-10T11:52:01.065266Z","iopub.status.idle":"2025-11-10T11:52:01.068855Z","shell.execute_reply.started":"2025-11-10T11:52:01.065238Z","shell.execute_reply":"2025-11-10T11:52:01.068179Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/shoplifting-videos-dataset/Shop DataSet\"\nOUTPUT_DIR = \"/kaggle/working\"\nNUM_FRAMES = 16\nFRAME_SIZE = 224\nBATCH_SIZE = 2\nEPOCHS = 3\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:52:03.756135Z","iopub.execute_input":"2025-11-10T11:52:03.756411Z","iopub.status.idle":"2025-11-10T11:52:03.760659Z","shell.execute_reply.started":"2025-11-10T11:52:03.756393Z","shell.execute_reply":"2025-11-10T11:52:03.759829Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"labels, corrupted = [], []\n\nsubdirs = [(\"shop lifters\", 1), (\"non shop lifters\", 0)]\nfor folder, label in subdirs:\n    folder_path = os.path.join(DATA_DIR, folder)\n    if not os.path.exists(folder_path):\n        continue\n    for video_file in os.listdir(folder_path):\n        if video_file.lower().endswith((\".mp4\", \".avi\", \".mov\", \".mkv\")):\n            video_path = os.path.join(folder_path, video_file)\n            vid = cv2.VideoCapture(video_path)\n            if not vid.isOpened():\n                corrupted.append(video_path)\n            else:\n                labels.append([video_path, label])\n            vid.release()\n\ndf = pd.DataFrame(labels, columns=[\"video_path\", \"label\"])\ndf.to_csv(os.path.join(OUTPUT_DIR, \"labels.csv\"), index=False)\nif corrupted:\n    print(f\"Corrupted videos: {len(corrupted)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:47:19.873635Z","iopub.execute_input":"2025-11-10T11:47:19.873886Z","iopub.status.idle":"2025-11-10T11:47:19.896810Z","shell.execute_reply.started":"2025-11-10T11:47:19.873865Z","shell.execute_reply":"2025-11-10T11:47:19.896138Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def check_empty_frames(video_path, threshold=10):\n    vid = cv2.VideoCapture(video_path)\n    frame_count = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n    empty_frames = 0\n    for i in range(min(100, frame_count)):\n        ret, frame = vid.read()\n        if ret and np.std(frame) < threshold:\n            empty_frames += 1\n    vid.release()\n    return empty_frames / min(100, frame_count) > 0.5\n\ndf_filtered = []\nfor i, row in df.iterrows():\n    if not check_empty_frames(row[\"video_path\"]):\n        df_filtered.append(row)\ndf = pd.DataFrame(df_filtered)\ndf.to_csv(os.path.join(OUTPUT_DIR, \"labels_final.csv\"), index=False)\nprint(f\"Videos after removing empty frames: {len(df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:47:19.897560Z","iopub.execute_input":"2025-11-10T11:47:19.898265Z","iopub.status.idle":"2025-11-10T11:47:19.915144Z","shell.execute_reply.started":"2025-11-10T11:47:19.898248Z","shell.execute_reply":"2025-11-10T11:47:19.914557Z"}},"outputs":[{"name":"stdout","text":"Videos after removing empty frames: 0\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"min_count = min(df[\"label\"].value_counts())\ndf_balanced = pd.concat([\n    df[df[\"label\"]==1].sample(min_count, random_state=42),\n    df[df[\"label\"]==0].sample(min_count, random_state=42)\n])\ndf_balanced.to_csv(os.path.join(OUTPUT_DIR, \"labels_balanced.csv\"), index=False)\nprint(f\"Balanced dataset size: {len(df_balanced)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:58:29.044514Z","iopub.execute_input":"2025-11-10T11:58:29.045103Z","iopub.status.idle":"2025-11-10T11:58:29.068423Z","shell.execute_reply.started":"2025-11-10T11:58:29.045075Z","shell.execute_reply":"2025-11-10T11:58:29.067534Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/2333898247.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m df_balanced = pd.concat([\n\u001b[1;32m      3\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ])\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHashable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_indexing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'label'"],"ename":"KeyError","evalue":"'label'","output_type":"error"}],"execution_count":33},{"cell_type":"code","source":"import os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    print(dirname)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:07:49.721902Z","iopub.execute_input":"2025-11-10T12:07:49.722641Z","iopub.status.idle":"2025-11-10T12:07:51.572831Z","shell.execute_reply.started":"2025-11-10T12:07:49.722615Z","shell.execute_reply":"2025-11-10T12:07:51.571949Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input\n/kaggle/input/Shop DataSet\n/kaggle/input/Shop DataSet/non shop lifters\n/kaggle/input/Shop DataSet/shop lifters\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"import os\nimport pandas as pd\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:08:51.463936Z","iopub.execute_input":"2025-11-10T12:08:51.464208Z","iopub.status.idle":"2025-11-10T12:08:51.468250Z","shell.execute_reply.started":"2025-11-10T12:08:51.464188Z","shell.execute_reply":"2025-11-10T12:08:51.467432Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"base_dir = \"/kaggle/input/Shop DataSet\"\n\nshoplifters_dir = os.path.join(base_dir, \"shop lifters\")\nnonshoplifters_dir = os.path.join(base_dir, \"non shop lifters\")\n\nprint(\"Shoplifters folder exists:\", os.path.exists(shoplifters_dir))\nprint(\"Non-shoplifters folder exists:\", os.path.exists(nonshoplifters_dir))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:09:03.225771Z","iopub.execute_input":"2025-11-10T12:09:03.226323Z","iopub.status.idle":"2025-11-10T12:09:03.235084Z","shell.execute_reply.started":"2025-11-10T12:09:03.226303Z","shell.execute_reply":"2025-11-10T12:09:03.234529Z"}},"outputs":[{"name":"stdout","text":"Shoplifters folder exists: True\nNon-shoplifters folder exists: True\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"shop_videos = [os.path.join(shoplifters_dir, f) for f in os.listdir(shoplifters_dir)]\nnonshop_videos = [os.path.join(nonshoplifters_dir, f) for f in os.listdir(nonshoplifters_dir)]\n\nprint(f\"Shoplifters videos: {len(shop_videos)}\")\nprint(f\"Non-shoplifters videos: {len(nonshop_videos)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:09:19.086285Z","iopub.execute_input":"2025-11-10T12:09:19.086580Z","iopub.status.idle":"2025-11-10T12:09:19.094277Z","shell.execute_reply.started":"2025-11-10T12:09:19.086559Z","shell.execute_reply":"2025-11-10T12:09:19.093725Z"}},"outputs":[{"name":"stdout","text":"Shoplifters videos: 324\nNon-shoplifters videos: 531\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"df = pd.DataFrame({\n    \"path\": shop_videos + nonshop_videos,\n    \"label\": [1] * len(shop_videos) + [0] * len(nonshop_videos)\n})\n\nprint(\" DataFrame created successfully!\")\nprint(\"Shape:\", df.shape)\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:09:41.468554Z","iopub.execute_input":"2025-11-10T12:09:41.468831Z","iopub.status.idle":"2025-11-10T12:09:41.499847Z","shell.execute_reply.started":"2025-11-10T12:09:41.468811Z","shell.execute_reply":"2025-11-10T12:09:41.499128Z"}},"outputs":[{"name":"stdout","text":" DataFrame created successfully!\nShape: (855, 2)\n","output_type":"stream"},{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"                                                path  label\n0  /kaggle/input/Shop DataSet/shop lifters/shop_l...      1\n1  /kaggle/input/Shop DataSet/shop lifters/videyy...      1\n2  /kaggle/input/Shop DataSet/shop lifters/shop_l...      1\n3  /kaggle/input/Shop DataSet/shop lifters/videyy...      1\n4  /kaggle/input/Shop DataSet/shop lifters/videpp...      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/Shop DataSet/shop lifters/shop_l...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/Shop DataSet/shop lifters/videyy...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/Shop DataSet/shop lifters/shop_l...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/Shop DataSet/shop lifters/videyy...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/Shop DataSet/shop lifters/videpp...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"min_count = df[\"label\"].value_counts().min()\n\ndf_balanced = pd.concat([\n    df[df[\"label\"] == 1].sample(min_count, random_state=42),\n    df[df[\"label\"] == 0].sample(min_count, random_state=42)\n])\n\nprint(\" Balanced dataset size:\", len(df_balanced))\ndf_balanced.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:10:24.814924Z","iopub.execute_input":"2025-11-10T12:10:24.815873Z","iopub.status.idle":"2025-11-10T12:10:24.868849Z","shell.execute_reply.started":"2025-11-10T12:10:24.815839Z","shell.execute_reply":"2025-11-10T12:10:24.868212Z"}},"outputs":[{"name":"stdout","text":" Balanced dataset size: 648\n","output_type":"stream"},{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"                                                  path  label\n132  /kaggle/input/Shop DataSet/shop lifters/videyy...      1\n108  /kaggle/input/Shop DataSet/shop lifters/videyy...      1\n137  /kaggle/input/Shop DataSet/shop lifters/videpp...      1\n9    /kaggle/input/Shop DataSet/shop lifters/videpp...      1\n180  /kaggle/input/Shop DataSet/shop lifters/shop_l...      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>132</th>\n      <td>/kaggle/input/Shop DataSet/shop lifters/videyy...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>108</th>\n      <td>/kaggle/input/Shop DataSet/shop lifters/videyy...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>137</th>\n      <td>/kaggle/input/Shop DataSet/shop lifters/videpp...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>/kaggle/input/Shop DataSet/shop lifters/videpp...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>180</th>\n      <td>/kaggle/input/Shop DataSet/shop lifters/shop_l...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"output_path = \"/kaggle/working/labels_balanced.csv\"\ndf_balanced.to_csv(output_path, index=False)\n\nprint(\" Saved balanced dataset to:\", output_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:10:46.275090Z","iopub.execute_input":"2025-11-10T12:10:46.275656Z","iopub.status.idle":"2025-11-10T12:10:46.282971Z","shell.execute_reply.started":"2025-11-10T12:10:46.275631Z","shell.execute_reply":"2025-11-10T12:10:46.282132Z"}},"outputs":[{"name":"stdout","text":" Saved balanced dataset to: /kaggle/working/labels_balanced.csv\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"df_check = pd.read_csv(output_path)\nprint(\" Reloaded balanced CSV successfully!\")\nprint(\"Shape:\", df_check.shape)\ndf_check.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:11:03.069368Z","iopub.execute_input":"2025-11-10T12:11:03.069656Z","iopub.status.idle":"2025-11-10T12:11:03.084956Z","shell.execute_reply.started":"2025-11-10T12:11:03.069635Z","shell.execute_reply":"2025-11-10T12:11:03.084342Z"}},"outputs":[{"name":"stdout","text":" Reloaded balanced CSV successfully!\nShape: (648, 2)\n","output_type":"stream"},{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"                                                path  label\n0  /kaggle/input/Shop DataSet/shop lifters/videyy...      1\n1  /kaggle/input/Shop DataSet/shop lifters/videyy...      1\n2  /kaggle/input/Shop DataSet/shop lifters/videpp...      1\n3  /kaggle/input/Shop DataSet/shop lifters/videpp...      1\n4  /kaggle/input/Shop DataSet/shop lifters/shop_l...      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/Shop DataSet/shop lifters/videyy...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/Shop DataSet/shop lifters/videyy...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/Shop DataSet/shop lifters/videpp...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/Shop DataSet/shop lifters/videpp...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/Shop DataSet/shop lifters/shop_l...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"# Cell 1\nimport os\nimport random\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\nimport torchvision.models as models\nimport cv2\nfrom tqdm.auto import tqdm\n\n# Paths (adjust if you moved dataset)\nBASE_DIR = \"/kaggle/input/Shop DataSet\"\nSHOP_DIR = os.path.join(BASE_DIR, \"shop lifters\")\nNONSHOP_DIR = os.path.join(BASE_DIR, \"non shop lifters\")\nOUTPUT_CSV = \"/kaggle/working/labels_all.csv\"\n\nprint(\"Exists:\", os.path.exists(BASE_DIR))\nprint(\"Shop lifters folder exists:\", os.path.exists(SHOP_DIR))\nprint(\"Non-shop lifters folder exists:\", os.path.exists(NONSHOP_DIR))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:14:19.182777Z","iopub.execute_input":"2025-11-10T12:14:19.183532Z","iopub.status.idle":"2025-11-10T12:14:19.194728Z","shell.execute_reply.started":"2025-11-10T12:14:19.183505Z","shell.execute_reply":"2025-11-10T12:14:19.194068Z"}},"outputs":[{"name":"stdout","text":"Exists: True\nShop lifters folder exists: True\nNon-shop lifters folder exists: True\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"# Cell 2\n# Build CSV of file paths + labels (1 = shoplifter, 0 = non-shoplifter)\nshop_files = sorted([os.path.join(SHOP_DIR, f) for f in os.listdir(SHOP_DIR) if not f.startswith('.')])\nnonshop_files = sorted([os.path.join(NONSHOP_DIR, f) for f in os.listdir(NONSHOP_DIR) if not f.startswith('.')])\n\nprint(\"Found shop files:\", len(shop_files))\nprint(\"Found non-shop files:\", len(nonshop_files))\n\ndf = pd.DataFrame({\n    \"path\": shop_files + nonshop_files,\n    \"label\": [1] * len(shop_files) + [0] * len(nonshop_files)\n})\n\ndf.to_csv(OUTPUT_CSV, index=False)\nprint(\"Saved CSV to:\", OUTPUT_CSV)\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:14:33.963679Z","iopub.execute_input":"2025-11-10T12:14:33.963951Z","iopub.status.idle":"2025-11-10T12:14:33.983264Z","shell.execute_reply.started":"2025-11-10T12:14:33.963928Z","shell.execute_reply":"2025-11-10T12:14:33.982435Z"}},"outputs":[{"name":"stdout","text":"Found shop files: 324\nFound non-shop files: 531\nSaved CSV to: /kaggle/working/labels_all.csv\n","output_type":"stream"},{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"                                                path  label\n0  /kaggle/input/Shop DataSet/shop lifters/shop_l...      1\n1  /kaggle/input/Shop DataSet/shop lifters/shop_l...      1\n2  /kaggle/input/Shop DataSet/shop lifters/shop_l...      1\n3  /kaggle/input/Shop DataSet/shop lifters/shop_l...      1\n4  /kaggle/input/Shop DataSet/shop lifters/shop_l...      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/Shop DataSet/shop lifters/shop_l...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/Shop DataSet/shop lifters/shop_l...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/Shop DataSet/shop lifters/shop_l...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/Shop DataSet/shop lifters/shop_l...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/Shop DataSet/shop lifters/shop_l...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"# Cell 3\ndf = pd.read_csv(OUTPUT_CSV)\ntrain_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n\ntrain_csv = \"/kaggle/working/train_labels.csv\"\nval_csv = \"/kaggle/working/val_labels.csv\"\ntrain_df.to_csv(train_csv, index=False)\nval_df.to_csv(val_csv, index=False)\n\nprint(\"Train size:\", len(train_df), \"Val size:\", len(val_df))\nprint(\"Train saved to:\", train_csv)\nprint(\"Val saved to:\", val_csv)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:14:45.478426Z","iopub.execute_input":"2025-11-10T12:14:45.478729Z","iopub.status.idle":"2025-11-10T12:14:45.505081Z","shell.execute_reply.started":"2025-11-10T12:14:45.478709Z","shell.execute_reply":"2025-11-10T12:14:45.504543Z"}},"outputs":[{"name":"stdout","text":"Train size: 684 Val size: 171\nTrain saved to: /kaggle/working/train_labels.csv\nVal saved to: /kaggle/working/val_labels.csv\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"# Cell 4\nclass VideoFramesDataset(Dataset):\n    \"\"\"\n    Reads a video file, samples `num_frames` frames uniformly,\n    applies transform to each frame, returns tensor: (num_frames, C, H, W)\n    \"\"\"\n    def __init__(self, csv_file, num_frames=8, transform=None, return_path=False):\n        self.df = pd.read_csv(csv_file)\n        self.num_frames = num_frames\n        self.transform = transform\n        self.return_path = return_path\n\n    def __len__(self):\n        return len(self.df)\n\n    def _read_video_frames(self, video_path, num_frames):\n        # Use OpenCV to read frames\n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            # attempt to read direct path fallback\n            raise RuntimeError(f\"Could not open video: {video_path}\")\n\n        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        if total <= 0:\n            # fallback: read until end to count\n            frames = []\n            while True:\n                ret, frame = cap.read()\n                if not ret:\n                    break\n                frames.append(frame)\n            cap.release()\n            total = len(frames)\n            if total == 0:\n                raise RuntimeError(f\"No frames in video: {video_path}\")\n            # sample indexes\n            idxs = np.linspace(0, total - 1, num_frames).astype(int)\n            sampled = [frames[i] for i in idxs]\n            return sampled\n\n        # choose uniform frame indices\n        idxs = np.linspace(0, max(total - 1, 0), num_frames).astype(int)\n        frames = []\n        for i in range(total):\n            ret, frame = cap.read()\n            if not ret:\n                break\n            if i in idxs:\n                frames.append(frame)\n                if len(frames) == num_frames:\n                    break\n        cap.release()\n        # if fewer frames captured (very short video), repeat last frame\n        if len(frames) < num_frames:\n            if len(frames) == 0:\n                raise RuntimeError(f\"No frames captured from: {video_path}\")\n            while len(frames) < num_frames:\n                frames.append(frames[-1].copy())\n        return frames\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        path = row[\"path\"]\n        label = int(row[\"label\"])\n\n        frames = self._read_video_frames(path, self.num_frames)  # list of BGR frames (H,W,C)\n\n        processed = []\n        for f in frames:\n            # convert BGR -> RGB\n            f = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n            if self.transform:\n                f = self.transform(image=f)[\"image\"] if hasattr(self.transform, '__call__') and isinstance(self.transform, dict) else self.transform(f)\n            processed.append(f)\n\n        # processed frames are expected to be PIL or tensor; let's ensure tensors (C,H,W)\n        # If transform returns numpy array HWC, convert to torch tensor\n        frames_tensor = torch.stack([torch.tensor(np.transpose(p, (2,0,1)), dtype=torch.float32) if isinstance(p, np.ndarray) else p for p in processed])\n        # normalize to [0,1] if necessary (some transforms already do)\n        if frames_tensor.max() > 5.0:  # likely 0-255\n            frames_tensor = frames_tensor / 255.0\n\n        if self.return_path:\n            return frames_tensor, label, path\n        return frames_tensor, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:14:58.087047Z","iopub.execute_input":"2025-11-10T12:14:58.087773Z","iopub.status.idle":"2025-11-10T12:14:58.100758Z","shell.execute_reply.started":"2025-11-10T12:14:58.087747Z","shell.execute_reply":"2025-11-10T12:14:58.100001Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"# Cell 5\n# Basic transforms: resize + center crop + normalization (for imagenet pretrained)\nIMG_SIZE = 224\nnum_frames = 8\nbatch_size = 8  # adjust to your GPU limits\n\n# Using torchvision transforms on PIL images is messy with cv2 arrays: we will use simple manual transform\nimport torchvision.transforms.functional as TF\n\ndef simple_frame_transform(frame_rgb):\n    # frame_rgb is numpy array HWC\n    img = cv2.resize(frame_rgb, (IMG_SIZE, IMG_SIZE))\n    img = img.astype(np.float32) / 255.0\n    # convert to tensor C,H,W\n    tensor = torch.tensor(np.transpose(img, (2,0,1)), dtype=torch.float32)\n    # normalize using ImageNet mean/std\n    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n    std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n    tensor = (tensor - mean) / std\n    return tensor\n\n# wrap transform so VideoFramesDataset uses it\ndef transform_wrapper(frame_rgb):\n    return simple_frame_transform(frame_rgb).numpy()  # convert to numpy so dataset will convert back to tensor\n\n# Create datasets\ntrain_dataset = VideoFramesDataset(train_csv, num_frames=num_frames, transform=lambda f: simple_frame_transform(f), return_path=False)\nval_dataset = VideoFramesDataset(val_csv, num_frames=num_frames, transform=lambda f: simple_frame_transform(f), return_path=False)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n\nprint(\"Train batches:\", len(train_loader), \"Val batches:\", len(val_loader))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:15:15.123795Z","iopub.execute_input":"2025-11-10T12:15:15.124665Z","iopub.status.idle":"2025-11-10T12:15:15.142831Z","shell.execute_reply.started":"2025-11-10T12:15:15.124625Z","shell.execute_reply":"2025-11-10T12:15:15.141969Z"}},"outputs":[{"name":"stdout","text":"Train batches: 86 Val batches: 22\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"# Cell 6\nclass FrameAggregationModel(nn.Module):\n    def __init__(self, backbone_name=\"resnet50\", pretrained=True, num_classes=2, dropout=0.5):\n        super().__init__()\n        # load resnet50 backbone\n        backbone = models.resnet50(pretrained=pretrained)\n        # remove classifier layer (fc)\n        self.feature_extractor = nn.Sequential(*list(backbone.children())[:-1])  # outputs (B,2048,1,1)\n        self.feature_dim = backbone.fc.in_features  # 2048 for resnet50\n        self.classifier = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(self.feature_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        # x: (B, num_frames, C, H, W)\n        B, F, C, H, W = x.shape\n        x = x.view(B * F, C, H, W)\n        feats = self.feature_extractor(x)  # (B*F, feat_dim, 1,1)\n        feats = feats.view(B, F, self.feature_dim)  # (B, F, feat_dim)\n        # average pooling across frames\n        feats = feats.mean(dim=1)  # (B, feat_dim)\n        out = self.classifier(feats)  # (B, num_classes)\n        return out\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = FrameAggregationModel(pretrained=True, num_classes=2).to(device)\nprint(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:15:28.810976Z","iopub.execute_input":"2025-11-10T12:15:28.811655Z","iopub.status.idle":"2025-11-10T12:15:30.121257Z","shell.execute_reply.started":"2025-11-10T12:15:28.811629Z","shell.execute_reply":"2025-11-10T12:15:30.120446Z"}},"outputs":[{"name":"stdout","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|| 97.8M/97.8M [00:00<00:00, 212MB/s]\n","output_type":"stream"},{"name":"stdout","text":"FrameAggregationModel(\n  (feature_extractor): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (5): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (6): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (4): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (5): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (7): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n  )\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=2048, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=512, out_features=2, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 7\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.5)\n\ndef train_one_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for batch in tqdm(loader):\n        inputs, labels = batch  # inputs: (B, F, C, H, W)\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n        preds = outputs.argmax(dim=1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n    return running_loss / total, correct / total\n\ndef validate(model, loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in tqdm(loader):\n            inputs, labels = batch\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * inputs.size(0)\n            preds = outputs.argmax(dim=1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n    return running_loss / total, correct / total\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:15:54.246635Z","iopub.execute_input":"2025-11-10T12:15:54.247262Z","iopub.status.idle":"2025-11-10T12:15:54.255600Z","shell.execute_reply.started":"2025-11-10T12:15:54.247240Z","shell.execute_reply":"2025-11-10T12:15:54.254804Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# Cell 8\nn_epochs = 8\nbest_val_acc = 0.0\ncheckpoint_path = \"/kaggle/working/best_model.pth\"\n\nfor epoch in range(1, n_epochs + 1):\n    print(f\"Epoch {epoch}/{n_epochs}\")\n    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n    val_loss, val_acc = validate(model, val_loader, criterion, device)\n    print(f\" Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f}\")\n    print(f\" Val   loss: {val_loss:.4f} | Val   acc: {val_acc:.4f}\")\n    # save best\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save({\n            \"model_state_dict\": model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n            \"epoch\": epoch,\n            \"val_acc\": val_acc\n        }, checkpoint_path)\n        print(\" Saved best model:\", checkpoint_path)\n    scheduler.step()\n\nprint(\"Training finished. Best val acc:\", best_val_acc)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:16:26.925711Z","iopub.execute_input":"2025-11-10T12:16:26.925991Z","iopub.status.idle":"2025-11-10T12:38:57.543602Z","shell.execute_reply.started":"2025-11-10T12:16:26.925971Z","shell.execute_reply":"2025-11-10T12:38:57.542612Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52321340bf5c4f11af341c2e694a16b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff3a58dc28ca45f084feed5cbcdc8230"}},"metadata":{}},{"name":"stdout","text":" Train loss: 0.2301 | Train acc: 0.9167\n Val   loss: 0.1251 | Val   acc: 0.9415\n Saved best model: /kaggle/working/best_model.pth\nEpoch 2/8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0d9fa3f1d954652b07c6688a9859ff8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba9408eec1e64f9fbb714569f1ab12ba"}},"metadata":{}},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b35127b47c0>\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b35127b47c0>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1600, in _shutdown_workers\n    self._pin_memory_thread.join()\n  File \"/usr/lib/python3.11/threading.py\", line 1116, in join\n    raise RuntimeError(\"cannot join current thread\")\nRuntimeError: cannot join current thread\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b35127b47c0>^\nTraceback (most recent call last):\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n^    ^self._shutdown_workers()^\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n^    if w.is_alive():^\n ^ \n  AssertionError :  can only test a child process \n^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7b35127b47c0>\nTraceback (most recent call last):\n^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n^    ^self._shutdown_workers()^\n^^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n^    ^if w.is_alive():\n\n   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n       assert self._parent_pid == os.getpid(), 'can only test a child process' \n      ^  ^  ^  ^^ ^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^  ^  ^ ^  ^^ ^ ^^  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^^AssertionError: ^^can only test a child process^\n^^^^^Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b35127b47c0>^\n^Traceback (most recent call last):\n\nAssertionError  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n:     can only test a child process\nself._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1637, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":" Train loss: 0.0642 | Train acc: 0.9795\n Val   loss: 0.2736 | Val   acc: 0.8830\nEpoch 3/8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24e40004b6a04e57be5686cfa4344304"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e716e6012af3469fb480d7100e6cd942"}},"metadata":{}},{"name":"stdout","text":" Train loss: 0.0373 | Train acc: 0.9883\n Val   loss: 0.0004 | Val   acc: 1.0000\n Saved best model: /kaggle/working/best_model.pth\nEpoch 4/8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"380141053b3c48edbd24a6af4e7bd870"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecd29ea0566c4261b5299282edb22ef0"}},"metadata":{}},{"name":"stdout","text":" Train loss: 0.0390 | Train acc: 0.9956\n Val   loss: 0.0042 | Val   acc: 1.0000\nEpoch 5/8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c55538c41da4978922587e3f9d7590e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88af4038dc784bb6b7c0c3628d9f124f"}},"metadata":{}},{"name":"stdout","text":" Train loss: 0.0665 | Train acc: 0.9781\n Val   loss: 0.0009 | Val   acc: 1.0000\nEpoch 6/8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e041422059d84c4eb15c244b01dfe96e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b05bccee119f4638835ebc04a6cee693"}},"metadata":{}},{"name":"stdout","text":" Train loss: 0.0051 | Train acc: 1.0000\n Val   loss: 0.0012 | Val   acc: 1.0000\nEpoch 7/8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0bb23f7d8fc4e5ea6999ac0b9dd7fc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aac1e8ed6b7d463fa4d7a5ef3da6020a"}},"metadata":{}},{"name":"stdout","text":" Train loss: 0.0360 | Train acc: 0.9839\n Val   loss: 0.0023 | Val   acc: 1.0000\nEpoch 8/8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/86 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"902ce529a6da4fcfb182eea6393a9ff5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f075ddca087343e1a714c5d2eb8d4930"}},"metadata":{}},{"name":"stdout","text":" Train loss: 0.0132 | Train acc: 0.9942\n Val   loss: 0.0005 | Val   acc: 1.0000\nTraining finished. Best val acc: 1.0\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"# Cell 9\ncheckpoint = torch.load(\"/kaggle/working/best_model.pth\", map_location=device)\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\nmodel.eval()\nprint(\"Loaded checkpoint from epoch\", checkpoint[\"epoch\"], \"val_acc\", checkpoint[\"val_acc\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:39:05.039810Z","iopub.execute_input":"2025-11-10T12:39:05.040624Z","iopub.status.idle":"2025-11-10T12:39:56.004234Z","shell.execute_reply.started":"2025-11-10T12:39:05.040586Z","shell.execute_reply":"2025-11-10T12:39:56.003525Z"}},"outputs":[{"name":"stdout","text":"Loaded checkpoint from epoch 3 val_acc 1.0\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"# Cell 10\nimport torch.nn.functional as F\n\ndef predict_video(model, video_path, num_frames=8, device=device):\n    model.eval()\n    ds = VideoFramesDataset(csv_file=None, num_frames=num_frames, transform=lambda f: simple_frame_transform(f), return_path=False)\n    # We will use the dataset internal frame reader directly to avoid CSV read\n    # copy of logic: read frames and transform\n    from pathlib import Path\n    frames = ds._read_video_frames(video_path, num_frames)\n    proc = [simple_frame_transform(cv2.cvtColor(f, cv2.COLOR_BGR2RGB)) for f in frames]  # each is tensor C,H,W\n    x = torch.stack(proc).unsqueeze(0)  # (1, F, C, H, W)\n    x = x.to(device)\n    with torch.no_grad():\n        out = model(x)\n        probs = F.softmax(out, dim=1).cpu().numpy()[0]\n    return {\"prob_nonshop\": float(probs[0]), \"prob_shop\": float(probs[1]), \"pred\": int(probs[1] > probs[0])}\n\n# quick test on a validation example\nsample_path = val_df.iloc[0][\"path\"]\nprint(\"Sample video:\", sample_path)\nprint(\"Prediction:\", predict_video(model, sample_path, num_frames=num_frames))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:40:40.238030Z","iopub.execute_input":"2025-11-10T12:40:40.238308Z","iopub.status.idle":"2025-11-10T12:40:40.346765Z","shell.execute_reply.started":"2025-11-10T12:40:40.238289Z","shell.execute_reply":"2025-11-10T12:40:40.345747Z"}},"outputs":[{"name":"stdout","text":"Sample video: /kaggle/input/Shop DataSet/non shop lifters/shop_lifter_n_122.mp4\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/2981929894.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0msample_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample video:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Prediction:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_48/2981929894.py\u001b[0m in \u001b[0;36mpredict_video\u001b[0;34m(model, video_path, num_frames, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoFramesDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msimple_frame_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# We will use the dataset internal frame reader directly to avoid CSV read\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# copy of logic: read frames and transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/3762645112.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, csv_file, num_frames, transform, return_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[0;31m# open URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m     ioargs = _get_filepath_or_buffer(\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    470\u001b[0m     ):\n\u001b[1;32m    471\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Invalid file path or buffer object type: {type(filepath_or_buffer)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     return IOArgs(\n","\u001b[0;31mValueError\u001b[0m: Invalid file path or buffer object type: <class 'NoneType'>"],"ename":"ValueError","evalue":"Invalid file path or buffer object type: <class 'NoneType'>","output_type":"error"}],"execution_count":60},{"cell_type":"code","source":"# Cell 11\n# NOTE: In Kaggle notebooks, the Gradio app may not open in the notebook output panel.\n# You can run this locally on your machine (after downloading the model and dataset) or try in Kaggle with `allow_flagging=False`.\ntry:\n    import gradio as gr\nexcept Exception:\n    !pip install -q gradio && python -m pip install -q gradio\n    import gradio as gr\n\ndef gradio_infer(video_file):\n    # video_file is a local path to uploaded video (temp)\n    res = predict_video(model, video_file.name if hasattr(video_file, 'name') else video_file, num_frames=num_frames)\n    label = \"shoplifter\" if res[\"pred\"] == 1 else \"non-shoplifter\"\n    return f\"Pred: {label} | prob_shop={res['prob_shop']:.3f} prob_nonshop={res['prob_nonshop']:.3f}\"\n\niface = gr.Interface(fn=gradio_infer, inputs=gr.Video(label=\"Upload video\"), outputs=\"text\",\n                     title=\"Shoplifting Detector (simple demo)\",\n                     description=\"Uploads a short video; model samples frames and predicts.\")\n# To run in notebook (it will try to open a server). In Kaggle this may not display; run locally instead.\niface.launch(share=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:41:16.010637Z","iopub.execute_input":"2025-11-10T12:41:16.011137Z","iopub.status.idle":"2025-11-10T12:41:20.818398Z","shell.execute_reply.started":"2025-11-10T12:41:16.011114Z","shell.execute_reply":"2025-11-10T12:41:20.817800Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* To create a public link, set `share=True` in `launch()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":61,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":61},{"cell_type":"code","source":"!pip install -q transformers accelerate timm gradio opencv-python\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:44:31.522847Z","iopub.execute_input":"2025-11-10T12:44:31.523593Z","iopub.status.idle":"2025-11-10T12:44:43.334127Z","shell.execute_reply.started":"2025-11-10T12:44:31.523563Z","shell.execute_reply":"2025-11-10T12:44:43.333137Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\nalbumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import VideoMAEForVideoClassification, VideoMAEImageProcessor\nimport torch.nn.functional as F\nimport gradio as gr\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:45:07.227695Z","iopub.execute_input":"2025-11-10T12:45:07.228251Z","iopub.status.idle":"2025-11-10T12:45:07.233982Z","shell.execute_reply.started":"2025-11-10T12:45:07.228219Z","shell.execute_reply":"2025-11-10T12:45:07.233286Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"BASE_DIR = \"/kaggle/input/Shop DataSet\"\nSHOP_DIR = os.path.join(BASE_DIR, \"shop lifters\")\nNONSHOP_DIR = os.path.join(BASE_DIR, \"non shop lifters\")\n\nshop_files = [os.path.join(SHOP_DIR, f) for f in os.listdir(SHOP_DIR) if f.endswith((\".mp4\", \".avi\"))]\nnonshop_files = [os.path.join(NONSHOP_DIR, f) for f in os.listdir(NONSHOP_DIR) if f.endswith((\".mp4\", \".avi\"))]\n\ndf = pd.DataFrame({\n    \"path\": shop_files + nonshop_files,\n    \"label\": [1]*len(shop_files) + [0]*len(nonshop_files)\n})\ntrain_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n\ntrain_csv = \"/kaggle/working/train.csv\"\nval_csv = \"/kaggle/working/val.csv\"\ntrain_df.to_csv(train_csv, index=False)\nval_df.to_csv(val_csv, index=False)\nprint(\"Train:\", len(train_df), \"Val:\", len(val_df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:45:22.431434Z","iopub.execute_input":"2025-11-10T12:45:22.431745Z","iopub.status.idle":"2025-11-10T12:45:22.461072Z","shell.execute_reply.started":"2025-11-10T12:45:22.431725Z","shell.execute_reply":"2025-11-10T12:45:22.460144Z"}},"outputs":[{"name":"stdout","text":"Train: 684 Val: 171\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"class VideoMAEDataset(Dataset):\n    def __init__(self, df, processor, num_frames=16, max_len=16):\n        self.df = df.reset_index(drop=True)\n        self.processor = processor\n        self.num_frames = num_frames\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.df)\n\n    def _read_video_frames(self, path, num_frames):\n        cap = cv2.VideoCapture(path)\n        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        if total <= 0:\n            raise RuntimeError(f\"Empty video: {path}\")\n        idxs = np.linspace(0, total - 1, num_frames).astype(int)\n        frames = []\n        for i in range(total):\n            ret, frame = cap.read()\n            if not ret:\n                break\n            if i in idxs:\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                frames.append(frame)\n        cap.release()\n        return frames\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        video_path = row[\"path\"]\n        label = int(row[\"label\"])\n        frames = self._read_video_frames(video_path, self.num_frames)\n        processed = self.processor(frames, return_tensors=\"pt\")\n        pixel_values = processed[\"pixel_values\"].squeeze(0)  # (num_frames, 3, H, W)\n        return {\"pixel_values\": pixel_values, \"labels\": torch.tensor(label)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:45:40.769716Z","iopub.execute_input":"2025-11-10T12:45:40.770474Z","iopub.status.idle":"2025-11-10T12:45:40.777890Z","shell.execute_reply.started":"2025-11-10T12:45:40.770445Z","shell.execute_reply":"2025-11-10T12:45:40.777164Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"model_name = \"MCG-NJU/videomae-base\"\nprocessor = VideoMAEImageProcessor.from_pretrained(model_name)\nmodel = VideoMAEForVideoClassification.from_pretrained(model_name, num_labels=2)\nmodel.to(DEVICE)\n\ntrain_dataset = VideoMAEDataset(train_df, processor, num_frames=16)\nval_dataset   = VideoMAEDataset(val_df, processor, num_frames=16)\n\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2)\nval_loader   = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:45:55.217529Z","iopub.execute_input":"2025-11-10T12:45:55.217931Z","iopub.status.idle":"2025-11-10T12:46:02.012369Z","shell.execute_reply.started":"2025-11-10T12:45:55.217901Z","shell.execute_reply":"2025-11-10T12:46:02.011770Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"256bea85735343b98a2546ac99ff4a92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59dd57436b8d45d48b609629eb4b2f55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/377M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c5ffefaaffd41a29ad438676ada8153"}},"metadata":{}},{"name":"stderr","text":"Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nepochs = 3\nbest_val_acc = 0.0\nsave_path = \"/kaggle/working/videomae_best.pth\"\n\nfor epoch in range(1, epochs+1):\n    model.train()\n    total_loss, correct, total = 0, 0, 0\n    for batch in tqdm(train_loader):\n        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n        labels = batch[\"labels\"].to(DEVICE)\n        outputs = model(pixel_values, labels=labels)\n        loss = outputs.loss\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * labels.size(0)\n        preds = outputs.logits.argmax(dim=1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n    train_acc = correct / total\n    train_loss = total_loss / total\n\n    # Validation\n    model.eval()\n    val_correct, val_total, val_loss = 0, 0, 0\n    with torch.no_grad():\n        for batch in val_loader:\n            pixel_values = batch[\"pixel_values\"].to(DEVICE)\n            labels = batch[\"labels\"].to(DEVICE)\n            outputs = model(pixel_values, labels=labels)\n            val_loss += outputs.loss.item() * labels.size(0)\n            preds = outputs.logits.argmax(dim=1)\n            val_correct += (preds == labels).sum().item()\n            val_total += labels.size(0)\n    val_acc = val_correct / val_total\n    val_loss = val_loss / val_total\n\n    print(f\"Epoch {epoch}: Train loss={train_loss:.4f} acc={train_acc:.4f} | Val loss={val_loss:.4f} acc={val_acc:.4f}\")\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), save_path)\n        print(\" Saved best model:\", save_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:46:22.282627Z","iopub.execute_input":"2025-11-10T12:46:22.283394Z","iopub.status.idle":"2025-11-10T13:08:13.304899Z","shell.execute_reply.started":"2025-11-10T12:46:22.283367Z","shell.execute_reply":"2025-11-10T13:08:13.303729Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/342 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a60664f26e2248bea3e9ced538724c0b"}},"metadata":{}},{"name":"stdout","text":"Epoch 1: Train loss=0.3758 acc=0.8450 | Val loss=0.1403 acc=0.9298\n Saved best model: /kaggle/working/videomae_best.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/342 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdb8f5103d8e451da327bb205e81cd98"}},"metadata":{}},{"name":"stdout","text":"Epoch 2: Train loss=0.1208 acc=0.9532 | Val loss=0.0191 acc=1.0000\n Saved best model: /kaggle/working/videomae_best.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/342 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c80b182139644707aeddbc3da2dd448a"}},"metadata":{}},{"name":"stdout","text":"Epoch 3: Train loss=0.0473 acc=0.9854 | Val loss=0.1244 acc=0.9532\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"def predict_video(video_path):\n    model.eval()\n    frames = VideoMAEDataset(train_df, processor)._read_video_frames(video_path, 16)\n    inputs = processor(frames, return_tensors=\"pt\").to(DEVICE)\n    with torch.no_grad():\n        outputs = model(**inputs)\n        probs = F.softmax(outputs.logits, dim=-1).cpu().numpy()[0]\n    pred = int(np.argmax(probs))\n    return {\"prob_nonshop\": float(probs[0]), \"prob_shop\": float(probs[1]), \"pred\": pred}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T13:22:26.507720Z","iopub.execute_input":"2025-11-10T13:22:26.508260Z","iopub.status.idle":"2025-11-10T13:22:26.512955Z","shell.execute_reply.started":"2025-11-10T13:22:26.508234Z","shell.execute_reply":"2025-11-10T13:22:26.512354Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"def gradio_predict(video_file):\n    result = predict_video(video_file.name)\n    label = \" Shoplifter\" if result[\"pred\"] == 1 else \" Non-Shoplifter\"\n    return f\"{label}\\nP(shoplifter)={result['prob_shop']:.3f}\\nP(non-shop)={result['prob_nonshop']:.3f}\"\n\nui = gr.Interface(\n    fn=gradio_predict,\n    inputs=gr.Video(label=\"Upload a short video\"),\n    outputs=\"text\",\n    title=\" Shoplifting Detection with VideoMAE\",\n    description=\"This fine-tuned VideoMAE model classifies short CCTV videos as shoplifter or non-shoplifter.\"\n)\n\nui.launch(share=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T13:33:34.742748Z","iopub.execute_input":"2025-11-10T13:33:34.743444Z","iopub.status.idle":"2025-11-10T13:33:36.068932Z","shell.execute_reply.started":"2025-11-10T13:33:34.743417Z","shell.execute_reply":"2025-11-10T13:33:36.068328Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7862\n* Running on public URL: https://2d9ad274a094eb52b8.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://2d9ad274a094eb52b8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":71,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":71},{"cell_type":"code","source":"def check_video_properties(video_path, min_frames=NUM_FRAMES, min_resolution=(FRAME_SIZE, FRAME_SIZE)):\n    vid = cv2.VideoCapture(video_path)\n    if not vid.isOpened():\n        return False, None\n    frame_count = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n    width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    vid.release()\n    if frame_count < min_frames or width < min_resolution[0] or height < min_resolution[1]:\n        return False, (frame_count, width, height)\n    return True, (frame_count, width, height)\n\ndf_cleaned = []\nfor i, row in df.iterrows():\n    valid, stats = check_video_properties(row[\"video_path\"])\n    if valid:\n        df_cleaned.append(row)\ndf = pd.DataFrame(df_cleaned)\ndf.to_csv(os.path.join(OUTPUT_DIR, \"labels_cleaned.csv\"), index=False)\nprint(f\"Videos after resolution/frame check: {len(df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:52:34.130958Z","iopub.execute_input":"2025-11-10T11:52:34.131596Z","iopub.status.idle":"2025-11-10T11:52:34.139093Z","shell.execute_reply.started":"2025-11-10T11:52:34.131569Z","shell.execute_reply":"2025-11-10T11:52:34.138542Z"}},"outputs":[{"name":"stdout","text":"Videos after resolution/frame check: 0\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"def check_empty_frames(video_path, threshold=10):\n    vid = cv2.VideoCapture(video_path)\n    frame_count = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n    empty_frames = 0\n    for i in range(min(100, frame_count)):\n        ret, frame = vid.read()\n        if ret and np.std(frame) < threshold:\n            empty_frames += 1\n    vid.release()\n    return empty_frames / min(100, frame_count) > 0.5\n\ndf_filtered = []\nfor i, row in df.iterrows():\n    if not check_empty_frames(row[\"video_path\"]):\n        df_filtered.append(row)\ndf = pd.DataFrame(df_filtered)\ndf.to_csv(os.path.join(OUTPUT_DIR, \"labels_final.csv\"), index=False)\nprint(f\"Videos after removing empty frames: {len(df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:52:41.118725Z","iopub.execute_input":"2025-11-10T11:52:41.119289Z","iopub.status.idle":"2025-11-10T11:52:41.126597Z","shell.execute_reply.started":"2025-11-10T11:52:41.119263Z","shell.execute_reply":"2025-11-10T11:52:41.125845Z"}},"outputs":[{"name":"stdout","text":"Videos after removing empty frames: 0\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"processor = VideoMAEImageProcessor.from_pretrained(MODEL_NAME)\n\nclass VideoMAEDataset(Dataset):\n    def __init__(self, df, num_frames=16):\n        self.df = df.reset_index(drop=True)\n        self.num_frames = num_frames\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        video_path = self.df.iloc[idx][\"video_path\"]\n        label = self.df.iloc[idx][\"label\"]\n\n        vr = VideoReader(video_path, ctx=cpu(0))\n        total_frames = len(vr)\n        indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n        frames = vr.get_batch(indices).asnumpy()  # (T, H, W, C)\n\n        inputs = processor(list(frames), return_tensors=\"pt\")\n        pixel_values = inputs[\"pixel_values\"].squeeze(0)\n        return pixel_values, torch.tensor(label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:53:01.114925Z","iopub.execute_input":"2025-11-10T11:53:01.115716Z","iopub.status.idle":"2025-11-10T11:53:01.137166Z","shell.execute_reply.started":"2025-11-10T11:53:01.115689Z","shell.execute_reply":"2025-11-10T11:53:01.136269Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/2009477458.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoMAEImageProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mVideoMAEDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'MODEL_NAME' is not defined"],"ename":"NameError","evalue":"name 'MODEL_NAME' is not defined","output_type":"error"}],"execution_count":31},{"cell_type":"code","source":"train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\ntrain_dataset = VideoMAEDataset(train_df)\nval_dataset = VideoMAEDataset(val_df)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:53:22.336464Z","iopub.execute_input":"2025-11-10T11:53:22.337102Z","iopub.status.idle":"2025-11-10T11:53:22.359512Z","shell.execute_reply.started":"2025-11-10T11:53:22.337077Z","shell.execute_reply":"2025-11-10T11:53:22.358540Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/4087818258.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoMAEDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoMAEDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHashable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_indexing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'label'"],"ename":"KeyError","evalue":"'label'","output_type":"error"}],"execution_count":32},{"cell_type":"code","source":"from transformers import VideoMAEForVideoClassification, VideoMAEImageProcessor\n\n# Pretrained model from Hugging Face\nmodel_name = \"MCG-NJU/videomae-base\"\nprocessor = VideoMAEImageProcessor.from_pretrained(model_name)\nmodel = VideoMAEForVideoClassification.from_pretrained(\n    model_name,\n    num_labels=2  # shoplifter / non-shoplifter\n)\nmodel = model.to(DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:47:20.181647Z","iopub.status.idle":"2025-11-10T11:47:20.181906Z","shell.execute_reply.started":"2025-11-10T11:47:20.181782Z","shell.execute_reply":"2025-11-10T11:47:20.181792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\ncriterion = nn.CrossEntropyLoss()\n\nfor epoch in range(EPOCHS):\n    model.train()\n    running_loss = 0\n    for pixel_values, labels in train_loader:\n        pixel_values, labels = pixel_values.to(DEVICE), labels.to(DEVICE)\n        outputs = model(pixel_values)\n        loss = criterion(outputs.logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {running_loss/len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:47:20.182643Z","iopub.status.idle":"2025-11-10T11:47:20.182847Z","shell.execute_reply.started":"2025-11-10T11:47:20.182748Z","shell.execute_reply":"2025-11-10T11:47:20.182757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\nall_labels, all_probs = [], []\n\nwith torch.no_grad():\n    for X, y in val_loader:\n        X, y = X.to(DEVICE), y.to(DEVICE)\n        outputs = model(pixel_values=X)\n        probs = torch.nn.functional.softmax(outputs.logits, dim=1)[:, 1]\n        all_probs.extend(probs.cpu().numpy())\n        all_labels.extend(y.cpu().numpy())\n\nall_labels = np.array(all_labels)\nall_probs = np.array(all_probs)\nall_preds = (all_probs >= 0.5).astype(int)\n\ncm = confusion_matrix(all_labels, all_preds)\nacc = accuracy_score(all_labels, all_preds)\nf1 = f1_score(all_labels, all_preds)\nprint(\"Confusion Matrix:\\n\", cm)\nprint(f\"Accuracy: {acc:.4f}, F1-score: {f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:47:20.183315Z","iopub.status.idle":"2025-11-10T11:47:20.183633Z","shell.execute_reply.started":"2025-11-10T11:47:20.183444Z","shell.execute_reply":"2025-11-10T11:47:20.183462Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi\n!nvcc --version\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:47:20.184844Z","iopub.status.idle":"2025-11-10T11:47:20.185098Z","shell.execute_reply.started":"2025-11-10T11:47:20.184990Z","shell.execute_reply":"2025-11-10T11:47:20.185000Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fpr, tpr, _ = roc_curve(all_labels, all_probs)\nroc_auc = roc_auc_score(all_labels, all_probs)\nplt.figure(figsize=(6,6))\nplt.plot(fpr, tpr, color='blue', lw=2, label=f\"AUC = {roc_auc:.4f}\")\nplt.plot([0,1],[0,1],'--', color='gray')\nplt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\"); plt.legend(); plt.grid(True); plt.show()\n\nprecision, recall, _ = precision_recall_curve(all_labels, all_probs)\npr_auc = auc(recall, precision)\nplt.figure(figsize=(6,6))\nplt.plot(recall, precision, color='green', lw=2, label=f\"PR AUC = {pr_auc:.4f}\")\nplt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\nplt.title(\"Precision-Recall Curve\"); plt.legend(); plt.grid(True); plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T11:47:20.185689Z","iopub.status.idle":"2025-11-10T11:47:20.185936Z","shell.execute_reply.started":"2025-11-10T11:47:20.185836Z","shell.execute_reply":"2025-11-10T11:47:20.185846Z"}},"outputs":[],"execution_count":null}]}